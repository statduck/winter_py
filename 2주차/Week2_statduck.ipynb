{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수 및 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수와 클래스는 긴밀히 연결되어 있어, 위키독스 순서를 무시했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n",
      "3\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "class Calculator: # 클래스\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "        \n",
    "    def add(self, num):\n",
    "        self.result += num\n",
    "        return self.result\n",
    "\n",
    "cal1 = Calculator() # 객체\n",
    "cal2 = Calculator()\n",
    "\n",
    "print(cal1.add(3))\n",
    "print(cal1.add(4))\n",
    "print(cal2.add(3))\n",
    "print(cal2.add(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클래스로 만든 객체: 인스턴스  \n",
    "a = Cookie() <-> a는 객체 <-> a는 Cookie의 인스턴스(관계 위주의 서술)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사칙역산 Cal 구현\n",
    "Class 구상  \n",
    "a = Fourcal()  \n",
    "a.setdata(4,2)  \n",
    "print(a.add(), a.mul(), a.sub(), a.div()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fourcal:\n",
    "    def setdata(self, first, second): #클래스 내부의 함수: 메서드\n",
    "        self.first = first # self에는 setdata를 호출한 객체 a가 자동전달.\n",
    "        self.second = second\n",
    "    def add(self):\n",
    "        result = self.first + self.second\n",
    "        return result\n",
    "    def mul(self):\n",
    "        result = self.first * self.second\n",
    "        return result\n",
    "    def sub(self):\n",
    "        result = self.first - self.second\n",
    "        return result\n",
    "    def div(self):\n",
    "        result = self.first / self.second\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Fourcal()\n",
    "a.setdata(4,2)\n",
    "a.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setdata보다는 __init__이 더 보편적.\n",
    "\n",
    "class Fourcal:\n",
    "    def __init__(self, first, second): #클래스 내부의 함수: 메서드\n",
    "        self.first = first # self에는 setdata를 호출한 객체 a가 자동전달.\n",
    "        self.second = second\n",
    "    def add(self):\n",
    "        result = self.first + self.second\n",
    "        return result\n",
    "    def mul(self):\n",
    "        result = self.first * self.second\n",
    "        return result\n",
    "    def sub(self):\n",
    "        result = self.first - self.second\n",
    "        return result\n",
    "    def div(self):\n",
    "        result = self.first / self.second\n",
    "        return result\n",
    "    \n",
    "a = Fourcal(4,2)\n",
    "a.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존클래스가 라이브러리형태이거나 수정이 허용되지 않은 상황 -> 상속이용\n",
    "class MoreFourCal(Fourcal): # 클래스확장.\n",
    "    def pow(self):\n",
    "        result = self.first ** self.second\n",
    "        return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = MoreFourCal(4,2)\n",
    "a.pow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/hunkim/PyTorchZeroToAll/blob/master/05_linear_regression.py\n",
    "# ref: https://wikidocs.net/60036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 78.72862243652344 \n",
      "Epoch: 1 | Loss: 35.04897689819336 \n",
      "Epoch: 2 | Loss: 15.604021072387695 \n",
      "Epoch: 3 | Loss: 6.9476637840271 \n",
      "Epoch: 4 | Loss: 3.09407901763916 \n",
      "Epoch: 5 | Loss: 1.3785570859909058 \n",
      "Epoch: 6 | Loss: 0.6148381233215332 \n",
      "Epoch: 7 | Loss: 0.27483677864074707 \n",
      "Epoch: 8 | Loss: 0.12346120923757553 \n",
      "Epoch: 9 | Loss: 0.056057170033454895 \n",
      "Epoch: 10 | Loss: 0.026035070419311523 \n",
      "Epoch: 11 | Loss: 0.012654445134103298 \n",
      "Epoch: 12 | Loss: 0.0066825710237026215 \n",
      "Epoch: 13 | Loss: 0.004008992575109005 \n",
      "Epoch: 14 | Loss: 0.0028039319440722466 \n",
      "Epoch: 15 | Loss: 0.002252804348245263 \n",
      "Epoch: 16 | Loss: 0.001993028447031975 \n",
      "Epoch: 17 | Loss: 0.001863132230937481 \n",
      "Epoch: 18 | Loss: 0.0017913070041686296 \n",
      "Epoch: 19 | Loss: 0.0017454945482313633 \n",
      "Epoch: 20 | Loss: 0.0017114740330725908 \n",
      "Epoch: 21 | Loss: 0.0016829029191285372 \n",
      "Epoch: 22 | Loss: 0.001656939391978085 \n",
      "Epoch: 23 | Loss: 0.0016323489835485816 \n",
      "Epoch: 24 | Loss: 0.0016085547395050526 \n",
      "Epoch: 25 | Loss: 0.0015852752840146422 \n",
      "Epoch: 26 | Loss: 0.0015624257503077388 \n",
      "Epoch: 27 | Loss: 0.0015399332623928785 \n",
      "Epoch: 28 | Loss: 0.0015178010798990726 \n",
      "Epoch: 29 | Loss: 0.001495963311754167 \n",
      "Epoch: 30 | Loss: 0.0014744828222319484 \n",
      "Epoch: 31 | Loss: 0.0014532905770465732 \n",
      "Epoch: 32 | Loss: 0.0014323899522423744 \n",
      "Epoch: 33 | Loss: 0.0014118205290287733 \n",
      "Epoch: 34 | Loss: 0.0013915051240473986 \n",
      "Epoch: 35 | Loss: 0.001371526625007391 \n",
      "Epoch: 36 | Loss: 0.0013518123887479305 \n",
      "Epoch: 37 | Loss: 0.0013323852326720953 \n",
      "Epoch: 38 | Loss: 0.0013132398016750813 \n",
      "Epoch: 39 | Loss: 0.0012943628244102001 \n",
      "Epoch: 40 | Loss: 0.0012757624499499798 \n",
      "Epoch: 41 | Loss: 0.0012574205175042152 \n",
      "Epoch: 42 | Loss: 0.0012393598444759846 \n",
      "Epoch: 43 | Loss: 0.001221542595885694 \n",
      "Epoch: 44 | Loss: 0.0012039814610034227 \n",
      "Epoch: 45 | Loss: 0.00118667958304286 \n",
      "Epoch: 46 | Loss: 0.001169609371572733 \n",
      "Epoch: 47 | Loss: 0.0011528177419677377 \n",
      "Epoch: 48 | Loss: 0.0011362623190507293 \n",
      "Epoch: 49 | Loss: 0.0011199306463822722 \n",
      "Epoch: 50 | Loss: 0.0011038128286600113 \n",
      "Epoch: 51 | Loss: 0.0010879479814320803 \n",
      "Epoch: 52 | Loss: 0.001072318060323596 \n",
      "Epoch: 53 | Loss: 0.0010569070000201464 \n",
      "Epoch: 54 | Loss: 0.001041712355799973 \n",
      "Epoch: 55 | Loss: 0.0010267490288242698 \n",
      "Epoch: 56 | Loss: 0.0010119827929884195 \n",
      "Epoch: 57 | Loss: 0.0009974529966711998 \n",
      "Epoch: 58 | Loss: 0.000983122969046235 \n",
      "Epoch: 59 | Loss: 0.0009689942235127091 \n",
      "Epoch: 60 | Loss: 0.0009550551185384393 \n",
      "Epoch: 61 | Loss: 0.0009413284133188426 \n",
      "Epoch: 62 | Loss: 0.0009278081706725061 \n",
      "Epoch: 63 | Loss: 0.0009144544019363821 \n",
      "Epoch: 64 | Loss: 0.0009013369563035667 \n",
      "Epoch: 65 | Loss: 0.0008883862756192684 \n",
      "Epoch: 66 | Loss: 0.0008756101597100496 \n",
      "Epoch: 67 | Loss: 0.0008630239171907306 \n",
      "Epoch: 68 | Loss: 0.0008506161975674331 \n",
      "Epoch: 69 | Loss: 0.0008384077227674425 \n",
      "Epoch: 70 | Loss: 0.0008263467461802065 \n",
      "Epoch: 71 | Loss: 0.0008144661551341414 \n",
      "Epoch: 72 | Loss: 0.000802772119641304 \n",
      "Epoch: 73 | Loss: 0.0007912378059700131 \n",
      "Epoch: 74 | Loss: 0.0007798519800417125 \n",
      "Epoch: 75 | Loss: 0.0007686485769227147 \n",
      "Epoch: 76 | Loss: 0.000757599831558764 \n",
      "Epoch: 77 | Loss: 0.0007467200048267841 \n",
      "Epoch: 78 | Loss: 0.0007359891314990819 \n",
      "Epoch: 79 | Loss: 0.0007254016818478703 \n",
      "Epoch: 80 | Loss: 0.0007149889133870602 \n",
      "Epoch: 81 | Loss: 0.0007047131075523794 \n",
      "Epoch: 82 | Loss: 0.0006945867789909244 \n",
      "Epoch: 83 | Loss: 0.0006846023024991155 \n",
      "Epoch: 84 | Loss: 0.0006747541483491659 \n",
      "Epoch: 85 | Loss: 0.0006650486611761153 \n",
      "Epoch: 86 | Loss: 0.0006555034779012203 \n",
      "Epoch: 87 | Loss: 0.000646085652988404 \n",
      "Epoch: 88 | Loss: 0.0006367991445586085 \n",
      "Epoch: 89 | Loss: 0.000627651228569448 \n",
      "Epoch: 90 | Loss: 0.0006186331156641245 \n",
      "Epoch: 91 | Loss: 0.0006097419536672533 \n",
      "Epoch: 92 | Loss: 0.0006009693606756628 \n",
      "Epoch: 93 | Loss: 0.0005923360004089773 \n",
      "Epoch: 94 | Loss: 0.0005838297656737268 \n",
      "Epoch: 95 | Loss: 0.0005754290032200515 \n",
      "Epoch: 96 | Loss: 0.0005671721883118153 \n",
      "Epoch: 97 | Loss: 0.000559016247279942 \n",
      "Epoch: 98 | Loss: 0.0005509800976142287 \n",
      "Epoch: 99 | Loss: 0.0005430590827018023 \n",
      "Epoch: 100 | Loss: 0.0005352573352865875 \n",
      "Epoch: 101 | Loss: 0.0005275687435641885 \n",
      "Epoch: 102 | Loss: 0.0005199814913794398 \n",
      "Epoch: 103 | Loss: 0.0005125111201778054 \n",
      "Epoch: 104 | Loss: 0.0005051522748544812 \n",
      "Epoch: 105 | Loss: 0.0004978920333087444 \n",
      "Epoch: 106 | Loss: 0.0004907262045890093 \n",
      "Epoch: 107 | Loss: 0.0004836705338675529 \n",
      "Epoch: 108 | Loss: 0.00047673002700321376 \n",
      "Epoch: 109 | Loss: 0.0004698731645476073 \n",
      "Epoch: 110 | Loss: 0.0004631209303624928 \n",
      "Epoch: 111 | Loss: 0.0004564679111354053 \n",
      "Epoch: 112 | Loss: 0.00044991480535827577 \n",
      "Epoch: 113 | Loss: 0.00044344644993543625 \n",
      "Epoch: 114 | Loss: 0.00043706383439712226 \n",
      "Epoch: 115 | Loss: 0.0004307823837734759 \n",
      "Epoch: 116 | Loss: 0.0004246009048074484 \n",
      "Epoch: 117 | Loss: 0.0004184991994407028 \n",
      "Epoch: 118 | Loss: 0.0004124817787669599 \n",
      "Epoch: 119 | Loss: 0.0004065501852892339 \n",
      "Epoch: 120 | Loss: 0.0004007051757071167 \n",
      "Epoch: 121 | Loss: 0.000394950999179855 \n",
      "Epoch: 122 | Loss: 0.000389268301660195 \n",
      "Epoch: 123 | Loss: 0.00038367684464901686 \n",
      "Epoch: 124 | Loss: 0.0003781603882089257 \n",
      "Epoch: 125 | Loss: 0.0003727281582541764 \n",
      "Epoch: 126 | Loss: 0.0003673698229249567 \n",
      "Epoch: 127 | Loss: 0.00036209452082403004 \n",
      "Epoch: 128 | Loss: 0.0003568826359696686 \n",
      "Epoch: 129 | Loss: 0.0003517630393616855 \n",
      "Epoch: 130 | Loss: 0.0003467031638137996 \n",
      "Epoch: 131 | Loss: 0.0003417186962906271 \n",
      "Epoch: 132 | Loss: 0.00033680812339298427 \n",
      "Epoch: 133 | Loss: 0.000331969145918265 \n",
      "Epoch: 134 | Loss: 0.00032719780574552715 \n",
      "Epoch: 135 | Loss: 0.0003224876127205789 \n",
      "Epoch: 136 | Loss: 0.00031785352621227503 \n",
      "Epoch: 137 | Loss: 0.00031328952172771096 \n",
      "Epoch: 138 | Loss: 0.0003087835793849081 \n",
      "Epoch: 139 | Loss: 0.0003043442266061902 \n",
      "Epoch: 140 | Loss: 0.00029998435638844967 \n",
      "Epoch: 141 | Loss: 0.0002956694515887648 \n",
      "Epoch: 142 | Loss: 0.0002914146170951426 \n",
      "Epoch: 143 | Loss: 0.00028722803108394146 \n",
      "Epoch: 144 | Loss: 0.00028310547349974513 \n",
      "Epoch: 145 | Loss: 0.0002790404250845313 \n",
      "Epoch: 146 | Loss: 0.0002750269486568868 \n",
      "Epoch: 147 | Loss: 0.00027106981724500656 \n",
      "Epoch: 148 | Loss: 0.0002671784022822976 \n",
      "Epoch: 149 | Loss: 0.0002633321564644575 \n",
      "Epoch: 150 | Loss: 0.00025954190641641617 \n",
      "Epoch: 151 | Loss: 0.000255818129517138 \n",
      "Epoch: 152 | Loss: 0.0002521448477637023 \n",
      "Epoch: 153 | Loss: 0.0002485205768607557 \n",
      "Epoch: 154 | Loss: 0.0002449457242619246 \n",
      "Epoch: 155 | Loss: 0.00024143156770151109 \n",
      "Epoch: 156 | Loss: 0.00023795716697350144 \n",
      "Epoch: 157 | Loss: 0.00023454398615285754 \n",
      "Epoch: 158 | Loss: 0.00023116964439395815 \n",
      "Epoch: 159 | Loss: 0.0002278416504850611 \n",
      "Epoch: 160 | Loss: 0.00022457048180513084 \n",
      "Epoch: 161 | Loss: 0.00022134633036330342 \n",
      "Epoch: 162 | Loss: 0.00021816481603309512 \n",
      "Epoch: 163 | Loss: 0.00021502550225704908 \n",
      "Epoch: 164 | Loss: 0.00021193687280174345 \n",
      "Epoch: 165 | Loss: 0.0002088924520649016 \n",
      "Epoch: 166 | Loss: 0.0002058925456367433 \n",
      "Epoch: 167 | Loss: 0.0002029317693086341 \n",
      "Epoch: 168 | Loss: 0.00020001750090159476 \n",
      "Epoch: 169 | Loss: 0.00019713983056135476 \n",
      "Epoch: 170 | Loss: 0.0001943113747984171 \n",
      "Epoch: 171 | Loss: 0.00019151381275150925 \n",
      "Epoch: 172 | Loss: 0.00018876171088777483 \n",
      "Epoch: 173 | Loss: 0.00018604738579597324 \n",
      "Epoch: 174 | Loss: 0.00018337747314944863 \n",
      "Epoch: 175 | Loss: 0.00018073587852995843 \n",
      "Epoch: 176 | Loss: 0.0001781406026566401 \n",
      "Epoch: 177 | Loss: 0.00017558143008500338 \n",
      "Epoch: 178 | Loss: 0.00017305338406004012 \n",
      "Epoch: 179 | Loss: 0.0001705672184471041 \n",
      "Epoch: 180 | Loss: 0.00016811939713079482 \n",
      "Epoch: 181 | Loss: 0.00016570572915952653 \n",
      "Epoch: 182 | Loss: 0.00016331659571733326 \n",
      "Epoch: 183 | Loss: 0.00016097392654046416 \n",
      "Epoch: 184 | Loss: 0.00015865515160840005 \n",
      "Epoch: 185 | Loss: 0.0001563833502586931 \n",
      "Epoch: 186 | Loss: 0.0001541348174214363 \n",
      "Epoch: 187 | Loss: 0.00015191725105978549 \n",
      "Epoch: 188 | Loss: 0.00014973245561122894 \n",
      "Epoch: 189 | Loss: 0.0001475871540606022 \n",
      "Epoch: 190 | Loss: 0.00014546856982633471 \n",
      "Epoch: 191 | Loss: 0.00014337261382024735 \n",
      "Epoch: 192 | Loss: 0.00014131501666270196 \n",
      "Epoch: 193 | Loss: 0.00013928084808867425 \n",
      "Epoch: 194 | Loss: 0.00013728051271755248 \n",
      "Epoch: 195 | Loss: 0.00013530244177673012 \n",
      "Epoch: 196 | Loss: 0.000133356501464732 \n",
      "Epoch: 197 | Loss: 0.00013144897820893675 \n",
      "Epoch: 198 | Loss: 0.00012955522106494755 \n",
      "Epoch: 199 | Loss: 0.00012769260501954705 \n",
      "Epoch: 200 | Loss: 0.00012585781223606318 \n",
      "Epoch: 201 | Loss: 0.000124048296129331 \n",
      "Epoch: 202 | Loss: 0.00012226120452396572 \n",
      "Epoch: 203 | Loss: 0.00012050494115101174 \n",
      "Epoch: 204 | Loss: 0.00011877469660248607 \n",
      "Epoch: 205 | Loss: 0.00011706953955581412 \n",
      "Epoch: 206 | Loss: 0.0001153873308794573 \n",
      "Epoch: 207 | Loss: 0.00011373218148946762 \n",
      "Epoch: 208 | Loss: 0.00011209237709408626 \n",
      "Epoch: 209 | Loss: 0.00011048403393942863 \n",
      "Epoch: 210 | Loss: 0.00010889762779697776 \n",
      "Epoch: 211 | Loss: 0.00010732811642810702 \n",
      "Epoch: 212 | Loss: 0.00010578794172033668 \n",
      "Epoch: 213 | Loss: 0.00010426453809486702 \n",
      "Epoch: 214 | Loss: 0.00010277045657858253 \n",
      "Epoch: 215 | Loss: 0.00010128979192813858 \n",
      "Epoch: 216 | Loss: 9.983903146348894e-05 \n",
      "Epoch: 217 | Loss: 9.839846461545676e-05 \n",
      "Epoch: 218 | Loss: 9.699010115582496e-05 \n",
      "Epoch: 219 | Loss: 9.559418685967103e-05 \n",
      "Epoch: 220 | Loss: 9.421994036529213e-05 \n",
      "Epoch: 221 | Loss: 9.286238491768017e-05 \n",
      "Epoch: 222 | Loss: 9.153135761152953e-05 \n",
      "Epoch: 223 | Loss: 9.021656296681613e-05 \n",
      "Epoch: 224 | Loss: 8.892362529877573e-05 \n",
      "Epoch: 225 | Loss: 8.764110680203885e-05 \n",
      "Epoch: 226 | Loss: 8.638249710202217e-05 \n",
      "Epoch: 227 | Loss: 8.513782813679427e-05 \n",
      "Epoch: 228 | Loss: 8.39170825202018e-05 \n",
      "Epoch: 229 | Loss: 8.270992839243263e-05 \n",
      "Epoch: 230 | Loss: 8.152201189659536e-05 \n",
      "Epoch: 231 | Loss: 8.034738129936159e-05 \n",
      "Epoch: 232 | Loss: 7.919938070699573e-05 \n",
      "Epoch: 233 | Loss: 7.805759378243238e-05 \n",
      "Epoch: 234 | Loss: 7.693428779020905e-05 \n",
      "Epoch: 235 | Loss: 7.58277383283712e-05 \n",
      "Epoch: 236 | Loss: 7.473523146472871e-05 \n",
      "Epoch: 237 | Loss: 7.366636418737471e-05 \n",
      "Epoch: 238 | Loss: 7.260966958710924e-05 \n",
      "Epoch: 239 | Loss: 7.15657661203295e-05 \n",
      "Epoch: 240 | Loss: 7.053699664538726e-05 \n",
      "Epoch: 241 | Loss: 6.952221156097949e-05 \n",
      "Epoch: 242 | Loss: 6.852368824183941e-05 \n",
      "Epoch: 243 | Loss: 6.754051719326526e-05 \n",
      "Epoch: 244 | Loss: 6.656562618445605e-05 \n",
      "Epoch: 245 | Loss: 6.561007467098534e-05 \n",
      "Epoch: 246 | Loss: 6.466959894169122e-05 \n",
      "Epoch: 247 | Loss: 6.373686483129859e-05 \n",
      "Epoch: 248 | Loss: 6.282471440499648e-05 \n",
      "Epoch: 249 | Loss: 6.192028376972303e-05 \n",
      "Epoch: 250 | Loss: 6.10294628131669e-05 \n",
      "Epoch: 251 | Loss: 6.015162216499448e-05 \n",
      "Epoch: 252 | Loss: 5.929066173848696e-05 \n",
      "Epoch: 253 | Loss: 5.844057523063384e-05 \n",
      "Epoch: 254 | Loss: 5.759928535553627e-05 \n",
      "Epoch: 255 | Loss: 5.677197259501554e-05 \n",
      "Epoch: 256 | Loss: 5.595260881818831e-05 \n",
      "Epoch: 257 | Loss: 5.51497760170605e-05 \n",
      "Epoch: 258 | Loss: 5.435788625618443e-05 \n",
      "Epoch: 259 | Loss: 5.357619738788344e-05 \n",
      "Epoch: 260 | Loss: 5.280713230604306e-05 \n",
      "Epoch: 261 | Loss: 5.20453104400076e-05 \n",
      "Epoch: 262 | Loss: 5.130400677444413e-05 \n",
      "Epoch: 263 | Loss: 5.056183726992458e-05 \n",
      "Epoch: 264 | Loss: 4.983655526302755e-05 \n",
      "Epoch: 265 | Loss: 4.911936412099749e-05 \n",
      "Epoch: 266 | Loss: 4.8414047341793776e-05 \n",
      "Epoch: 267 | Loss: 4.771945532411337e-05 \n",
      "Epoch: 268 | Loss: 4.703646118286997e-05 \n",
      "Epoch: 269 | Loss: 4.6359382395166904e-05 \n",
      "Epoch: 270 | Loss: 4.56931084045209e-05 \n",
      "Epoch: 271 | Loss: 4.50326333520934e-05 \n",
      "Epoch: 272 | Loss: 4.438645555637777e-05 \n",
      "Epoch: 273 | Loss: 4.37520575360395e-05 \n",
      "Epoch: 274 | Loss: 4.3121850467287004e-05 \n",
      "Epoch: 275 | Loss: 4.250152414897457e-05 \n",
      "Epoch: 276 | Loss: 4.1893024899763986e-05 \n",
      "Epoch: 277 | Loss: 4.129208900849335e-05 \n",
      "Epoch: 278 | Loss: 4.0696613723412156e-05 \n",
      "Epoch: 279 | Loss: 4.011374039691873e-05 \n",
      "Epoch: 280 | Loss: 3.953544364776462e-05 \n",
      "Epoch: 281 | Loss: 3.896679118042812e-05 \n",
      "Epoch: 282 | Loss: 3.840693534584716e-05 \n",
      "Epoch: 283 | Loss: 3.785614171647467e-05 \n",
      "Epoch: 284 | Loss: 3.7307199818314984e-05 \n",
      "Epoch: 285 | Loss: 3.677160566439852e-05 \n",
      "Epoch: 286 | Loss: 3.6244433431420475e-05 \n",
      "Epoch: 287 | Loss: 3.5724195186048746e-05 \n",
      "Epoch: 288 | Loss: 3.5212891816627234e-05 \n",
      "Epoch: 289 | Loss: 3.4706987207755446e-05 \n",
      "Epoch: 290 | Loss: 3.420576831558719e-05 \n",
      "Epoch: 291 | Loss: 3.371325874468312e-05 \n",
      "Epoch: 292 | Loss: 3.32286799675785e-05 \n",
      "Epoch: 293 | Loss: 3.274943446740508e-05 \n",
      "Epoch: 294 | Loss: 3.227862544008531e-05 \n",
      "Epoch: 295 | Loss: 3.181614010827616e-05 \n",
      "Epoch: 296 | Loss: 3.135878432658501e-05 \n",
      "Epoch: 297 | Loss: 3.090700920438394e-05 \n",
      "Epoch: 298 | Loss: 3.0464445444522426e-05 \n",
      "Epoch: 299 | Loss: 3.002571247634478e-05 \n",
      "Epoch: 300 | Loss: 2.959426819870714e-05 \n",
      "Epoch: 301 | Loss: 2.9170187190175056e-05 \n",
      "Epoch: 302 | Loss: 2.8749482225975953e-05 \n",
      "Epoch: 303 | Loss: 2.8336009563645348e-05 \n",
      "Epoch: 304 | Loss: 2.7928452254855074e-05 \n",
      "Epoch: 305 | Loss: 2.7529029466677457e-05 \n",
      "Epoch: 306 | Loss: 2.713354115257971e-05 \n",
      "Epoch: 307 | Loss: 2.6743629859993234e-05 \n",
      "Epoch: 308 | Loss: 2.6357134629506618e-05 \n",
      "Epoch: 309 | Loss: 2.597730235720519e-05 \n",
      "Epoch: 310 | Loss: 2.5606106646591797e-05 \n",
      "Epoch: 311 | Loss: 2.523729926906526e-05 \n",
      "Epoch: 312 | Loss: 2.4873195798136294e-05 \n",
      "Epoch: 313 | Loss: 2.451648106216453e-05 \n",
      "Epoch: 314 | Loss: 2.416377537883818e-05 \n",
      "Epoch: 315 | Loss: 2.3818025510990992e-05 \n",
      "Epoch: 316 | Loss: 2.347349982301239e-05 \n",
      "Epoch: 317 | Loss: 2.3139171389630064e-05 \n",
      "Epoch: 318 | Loss: 2.2804886611993425e-05 \n",
      "Epoch: 319 | Loss: 2.2477439415524714e-05 \n",
      "Epoch: 320 | Loss: 2.215386848547496e-05 \n",
      "Epoch: 321 | Loss: 2.183576361858286e-05 \n",
      "Epoch: 322 | Loss: 2.1523603209061548e-05 \n",
      "Epoch: 323 | Loss: 2.1212084902799688e-05 \n",
      "Epoch: 324 | Loss: 2.09101453947369e-05 \n",
      "Epoch: 325 | Loss: 2.0608131308108568e-05 \n",
      "Epoch: 326 | Loss: 2.0312503693276085e-05 \n",
      "Epoch: 327 | Loss: 2.0020966985612176e-05 \n",
      "Epoch: 328 | Loss: 1.97336012206506e-05 \n",
      "Epoch: 329 | Loss: 1.9447674276307225e-05 \n",
      "Epoch: 330 | Loss: 1.916955443448387e-05 \n",
      "Epoch: 331 | Loss: 1.8895210814662278e-05 \n",
      "Epoch: 332 | Loss: 1.8622969946591184e-05 \n",
      "Epoch: 333 | Loss: 1.8354077838012017e-05 \n",
      "Epoch: 334 | Loss: 1.8089984223479405e-05 \n",
      "Epoch: 335 | Loss: 1.7829886928666383e-05 \n",
      "Epoch: 336 | Loss: 1.7574478988535702e-05 \n",
      "Epoch: 337 | Loss: 1.73218813870335e-05 \n",
      "Epoch: 338 | Loss: 1.7073030903702602e-05 \n",
      "Epoch: 339 | Loss: 1.6829204469104297e-05 \n",
      "Epoch: 340 | Loss: 1.658488326938823e-05 \n",
      "Epoch: 341 | Loss: 1.6345991753041744e-05 \n",
      "Epoch: 342 | Loss: 1.611221523489803e-05 \n",
      "Epoch: 343 | Loss: 1.588093255122658e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 344 | Loss: 1.5651787180104293e-05 \n",
      "Epoch: 345 | Loss: 1.5426241589011624e-05 \n",
      "Epoch: 346 | Loss: 1.5203587281575892e-05 \n",
      "Epoch: 347 | Loss: 1.4985808775236364e-05 \n",
      "Epoch: 348 | Loss: 1.4772170288779307e-05 \n",
      "Epoch: 349 | Loss: 1.455918300052872e-05 \n",
      "Epoch: 350 | Loss: 1.4349171578942332e-05 \n",
      "Epoch: 351 | Loss: 1.4145059139991645e-05 \n",
      "Epoch: 352 | Loss: 1.3940672943135723e-05 \n",
      "Epoch: 353 | Loss: 1.3740361282543745e-05 \n",
      "Epoch: 354 | Loss: 1.3542889064410701e-05 \n",
      "Epoch: 355 | Loss: 1.3347696949495003e-05 \n",
      "Epoch: 356 | Loss: 1.315729423367884e-05 \n",
      "Epoch: 357 | Loss: 1.2967109796591103e-05 \n",
      "Epoch: 358 | Loss: 1.2779972166754305e-05 \n",
      "Epoch: 359 | Loss: 1.259749387827469e-05 \n",
      "Epoch: 360 | Loss: 1.2416841855156235e-05 \n",
      "Epoch: 361 | Loss: 1.223790332005592e-05 \n",
      "Epoch: 362 | Loss: 1.2061272173014004e-05 \n",
      "Epoch: 363 | Loss: 1.188822807307588e-05 \n",
      "Epoch: 364 | Loss: 1.171812618849799e-05 \n",
      "Epoch: 365 | Loss: 1.1550044291652739e-05 \n",
      "Epoch: 366 | Loss: 1.138405696110567e-05 \n",
      "Epoch: 367 | Loss: 1.122092544392217e-05 \n",
      "Epoch: 368 | Loss: 1.105935916712042e-05 \n",
      "Epoch: 369 | Loss: 1.0899922926910222e-05 \n",
      "Epoch: 370 | Loss: 1.0742694939835928e-05 \n",
      "Epoch: 371 | Loss: 1.0589731573418248e-05 \n",
      "Epoch: 372 | Loss: 1.0437581295263954e-05 \n",
      "Epoch: 373 | Loss: 1.028662700264249e-05 \n",
      "Epoch: 374 | Loss: 1.0137975550605915e-05 \n",
      "Epoch: 375 | Loss: 9.991597835323773e-06 \n",
      "Epoch: 376 | Loss: 9.848474292084575e-06 \n",
      "Epoch: 377 | Loss: 9.707562639960088e-06 \n",
      "Epoch: 378 | Loss: 9.569193935021758e-06 \n",
      "Epoch: 379 | Loss: 9.43190843827324e-06 \n",
      "Epoch: 380 | Loss: 9.294994924857747e-06 \n",
      "Epoch: 381 | Loss: 9.16233693715185e-06 \n",
      "Epoch: 382 | Loss: 9.030633009388112e-06 \n",
      "Epoch: 383 | Loss: 8.899710337573197e-06 \n",
      "Epoch: 384 | Loss: 8.772843102633487e-06 \n",
      "Epoch: 385 | Loss: 8.646372407383751e-06 \n",
      "Epoch: 386 | Loss: 8.522264579369221e-06 \n",
      "Epoch: 387 | Loss: 8.398463251069188e-06 \n",
      "Epoch: 388 | Loss: 8.278661880467553e-06 \n",
      "Epoch: 389 | Loss: 8.159471690305509e-06 \n",
      "Epoch: 390 | Loss: 8.04221781436354e-06 \n",
      "Epoch: 391 | Loss: 7.927203114377335e-06 \n",
      "Epoch: 392 | Loss: 7.812530384398997e-06 \n",
      "Epoch: 393 | Loss: 7.701354661548976e-06 \n",
      "Epoch: 394 | Loss: 7.591296707687434e-06 \n",
      "Epoch: 395 | Loss: 7.481554348487407e-06 \n",
      "Epoch: 396 | Loss: 7.3734008765313774e-06 \n",
      "Epoch: 397 | Loss: 7.267601176863536e-06 \n",
      "Epoch: 398 | Loss: 7.16210161044728e-06 \n",
      "Epoch: 399 | Loss: 7.060612915665843e-06 \n",
      "Epoch: 400 | Loss: 6.957705409149639e-06 \n",
      "Epoch: 401 | Loss: 6.858746019133832e-06 \n",
      "Epoch: 402 | Loss: 6.76057379678241e-06 \n",
      "Epoch: 403 | Loss: 6.662660325673642e-06 \n",
      "Epoch: 404 | Loss: 6.567619038833072e-06 \n",
      "Epoch: 405 | Loss: 6.473335815826431e-06 \n",
      "Epoch: 406 | Loss: 6.379294973157812e-06 \n",
      "Epoch: 407 | Loss: 6.289220436883625e-06 \n",
      "Epoch: 408 | Loss: 6.197327365953242e-06 \n",
      "Epoch: 409 | Loss: 6.109341484261677e-06 \n",
      "Epoch: 410 | Loss: 6.021557965141255e-06 \n",
      "Epoch: 411 | Loss: 5.934481123404112e-06 \n",
      "Epoch: 412 | Loss: 5.848459295521025e-06 \n",
      "Epoch: 413 | Loss: 5.765368769061752e-06 \n",
      "Epoch: 414 | Loss: 5.68384348298423e-06 \n",
      "Epoch: 415 | Loss: 5.6002863857429475e-06 \n",
      "Epoch: 416 | Loss: 5.520418199012056e-06 \n",
      "Epoch: 417 | Loss: 5.440719178295694e-06 \n",
      "Epoch: 418 | Loss: 5.3633475545211695e-06 \n",
      "Epoch: 419 | Loss: 5.285060979076661e-06 \n",
      "Epoch: 420 | Loss: 5.208479706197977e-06 \n",
      "Epoch: 421 | Loss: 5.135417268320452e-06 \n",
      "Epoch: 422 | Loss: 5.060192506789463e-06 \n",
      "Epoch: 423 | Loss: 4.988181899534538e-06 \n",
      "Epoch: 424 | Loss: 4.917590558761731e-06 \n",
      "Epoch: 425 | Loss: 4.84635120301391e-06 \n",
      "Epoch: 426 | Loss: 4.775442448590184e-06 \n",
      "Epoch: 427 | Loss: 4.707702828454785e-06 \n",
      "Epoch: 428 | Loss: 4.640072347683599e-06 \n",
      "Epoch: 429 | Loss: 4.572559191728942e-06 \n",
      "Epoch: 430 | Loss: 4.508130587055348e-06 \n",
      "Epoch: 431 | Loss: 4.443976649781689e-06 \n",
      "Epoch: 432 | Loss: 4.379005076771136e-06 \n",
      "Epoch: 433 | Loss: 4.315299065638101e-06 \n",
      "Epoch: 434 | Loss: 4.254154191585258e-06 \n",
      "Epoch: 435 | Loss: 4.192555934423581e-06 \n",
      "Epoch: 436 | Loss: 4.132703907089308e-06 \n",
      "Epoch: 437 | Loss: 4.074105163454078e-06 \n",
      "Epoch: 438 | Loss: 4.015575541416183e-06 \n",
      "Epoch: 439 | Loss: 3.957123226427939e-06 \n",
      "Epoch: 440 | Loss: 3.900590854755137e-06 \n",
      "Epoch: 441 | Loss: 3.844123966700863e-06 \n",
      "Epoch: 442 | Loss: 3.788464937315439e-06 \n",
      "Epoch: 443 | Loss: 3.734614892891841e-06 \n",
      "Epoch: 444 | Loss: 3.6819321849179687e-06 \n",
      "Epoch: 445 | Loss: 3.628626927820733e-06 \n",
      "Epoch: 446 | Loss: 3.576040853658924e-06 \n",
      "Epoch: 447 | Loss: 3.525092779455008e-06 \n",
      "Epoch: 448 | Loss: 3.4743482046906138e-06 \n",
      "Epoch: 449 | Loss: 3.424348278713296e-06 \n",
      "Epoch: 450 | Loss: 3.3747653560567414e-06 \n",
      "Epoch: 451 | Loss: 3.3261781027249526e-06 \n",
      "Epoch: 452 | Loss: 3.2788386761239963e-06 \n",
      "Epoch: 453 | Loss: 3.231526079616742e-06 \n",
      "Epoch: 454 | Loss: 3.185541118000401e-06 \n",
      "Epoch: 455 | Loss: 3.1397835300595034e-06 \n",
      "Epoch: 456 | Loss: 3.0950220661907224e-06 \n",
      "Epoch: 457 | Loss: 3.0501257697324036e-06 \n",
      "Epoch: 458 | Loss: 3.006513679792988e-06 \n",
      "Epoch: 459 | Loss: 2.962916141768801e-06 \n",
      "Epoch: 460 | Loss: 2.9201339657447534e-06 \n",
      "Epoch: 461 | Loss: 2.8783535981347086e-06 \n",
      "Epoch: 462 | Loss: 2.8368244784360286e-06 \n",
      "Epoch: 463 | Loss: 2.7964240416622488e-06 \n",
      "Epoch: 464 | Loss: 2.7566511562326923e-06 \n",
      "Epoch: 465 | Loss: 2.7167322969035013e-06 \n",
      "Epoch: 466 | Loss: 2.677580823728931e-06 \n",
      "Epoch: 467 | Loss: 2.6397510737297125e-06 \n",
      "Epoch: 468 | Loss: 2.6010193323600106e-06 \n",
      "Epoch: 469 | Loss: 2.5632261895225383e-06 \n",
      "Epoch: 470 | Loss: 2.526400066926726e-06 \n",
      "Epoch: 471 | Loss: 2.490299948476604e-06 \n",
      "Epoch: 472 | Loss: 2.454369450788363e-06 \n",
      "Epoch: 473 | Loss: 2.41883572016377e-06 \n",
      "Epoch: 474 | Loss: 2.38472694036318e-06 \n",
      "Epoch: 475 | Loss: 2.3513071027991828e-06 \n",
      "Epoch: 476 | Loss: 2.316839527338743e-06 \n",
      "Epoch: 477 | Loss: 2.2832414288131986e-06 \n",
      "Epoch: 478 | Loss: 2.250892066513188e-06 \n",
      "Epoch: 479 | Loss: 2.219206635345472e-06 \n",
      "Epoch: 480 | Loss: 2.1864991595066385e-06 \n",
      "Epoch: 481 | Loss: 2.1555272269324632e-06 \n",
      "Epoch: 482 | Loss: 2.1240148271317594e-06 \n",
      "Epoch: 483 | Loss: 2.0933637188136345e-06 \n",
      "Epoch: 484 | Loss: 2.0643142306653317e-06 \n",
      "Epoch: 485 | Loss: 2.0343068172223866e-06 \n",
      "Epoch: 486 | Loss: 2.004765974561451e-06 \n",
      "Epoch: 487 | Loss: 1.9760136638069525e-06 \n",
      "Epoch: 488 | Loss: 1.9481178696878487e-06 \n",
      "Epoch: 489 | Loss: 1.92017841982306e-06 \n",
      "Epoch: 490 | Loss: 1.8926809843833325e-06 \n",
      "Epoch: 491 | Loss: 1.8649054709385382e-06 \n",
      "Epoch: 492 | Loss: 1.8386360807198798e-06 \n",
      "Epoch: 493 | Loss: 1.8120451841241447e-06 \n",
      "Epoch: 494 | Loss: 1.7854912357506691e-06 \n",
      "Epoch: 495 | Loss: 1.7596732959646033e-06 \n",
      "Epoch: 496 | Loss: 1.7347704215353588e-06 \n",
      "Epoch: 497 | Loss: 1.7099698652600637e-06 \n",
      "Epoch: 498 | Loss: 1.6847059214342153e-06 \n",
      "Epoch: 499 | Loss: 1.661352371229441e-06 \n",
      "Prediction (after training) 4 8.001482009887695\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "x_data = tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__() # super(): nn.Module 클래스의 속성들을 가지고 초기화\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out: input_dim=1, output_dim=1\n",
    "        # 클래스를 확장하여, 내가 원하는 모델을 만드는 것이다.\n",
    "\n",
    "    def forward(self, x): # forward() 함수는 model 객체를 데이터와 함께 호출하면 자동으로 실행됨.\n",
    "        \"\"\" \n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model() # nn.Module 클래스 확장\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # 2) Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training\n",
    "hour_var = tensor([[4.0]])\n",
    "y_pred = model(hour_var)\n",
    "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://m.blog.naver.com/PostView.nhn?blogId=rlaghlfh&logNo=220914107525&proxyReferer=https:%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor of rank 0: Scalar  \n",
    "Tensor of rank 1: Vector  \n",
    "Tensor of rank 2: Dyad\n",
    "Tensor of rank 3: Triad  \n",
    "...  \n",
    "\n",
    "Torch에서 텐서는 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data # 현재 1*3 구조이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9989],\n",
       "        [3.9998],\n",
       "        [6.0006]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "class Calculator:\n",
    "    def __init__(self):\n",
    "        self.value = 0\n",
    "\n",
    "    def add(self, val):\n",
    "        self.value += val\n",
    "\n",
    "class UpgradeCalculator(Calculator):\n",
    "    def minus(self, val):\n",
    "        self.value -= val\n",
    "    \n",
    "cal = UpgradeCalculator()\n",
    "cal.add(10)\n",
    "cal.minus(7)\n",
    "\n",
    "print(cal.value) # 10에서 7을 뺀 3을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Q2: Method overriding       \n",
    "class MaxLimitCalculator(Calculator): \n",
    "    def add(self, val):\n",
    "        self.value += val\n",
    "        if self.value >= 100:\n",
    "            self.value = 100\n",
    "        else:\n",
    "            pass\n",
    "cal = MaxLimitCalculator()\n",
    "cal.add(50) # 50 더하기\n",
    "cal.add(60) # 60 더하기\n",
    "\n",
    "print(cal.value) # 100 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "# Q3\n",
    "print(all([1, 2, abs(-3)-3]),chr(ord('a')) == 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 8]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "list(filter(lambda x: x>0, [1, -2, 3, -5, 8, -3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "int(hex(234),16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 9, 12]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "list(map(lambda x:3*x, [1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "data = [-8, 2, 7, 5, -3, 5, 0, 1]\n",
    "min(data)+max(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.6667"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q8\n",
    "round(5.666666666666667,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
